{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coronavirus - Estimating Case Fatality Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, re, pickle\n",
    "\n",
    "from pydemic import Pandemic, Outbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "plt.rcParams['figure.figsize']=[40,20]\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronavirus_confirmed_df = pd.read_csv(\"../data/clean/coronavirus_confirmed_global.csv\", index_col=0)\n",
    "coronavirus_death_df = pd.read_csv(\"../data/clean/coronavirus_death_global.csv\", index_col=0)\n",
    "coronavirus_recovered_df = pd.read_csv(\"../data/clean/coronavirus_recovered_global.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandemic = Pandemic(\"Coronavirus\", coronavirus_confirmed_df, coronavirus_death_df, coronavirus_recovered_df)\n",
    "pandemic.set_smoothing_coefficient(3)\n",
    "top10_countries = pandemic.get_top_regions(top_n=10, exclude=[\"China\", \"Iran\"])\n",
    "top20_countries = pandemic.get_top_regions(top_n=20, exclude=[\"United Kingdom\", \"Iran\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region, outbreak in pandemic.outbreaks.items():\n",
    "    outbreak.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars_df = pd.read_csv(\"../data/time_series/diff/SARS.csv\", index_col=0, parse_dates=[0])\n",
    "sars_outbreak = Outbreak(\"SARS\", sars_df.Infected, sars_df.Dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimator_curve(outbreak, estimator, **kwargs):\n",
    "    curve = [estimator(outbreak, t) for t in range(outbreak.duration)]\n",
    "    \n",
    "    plt.plot(curve, label=outbreak.region, **kwargs)\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive CFR\n",
    "\n",
    "This is the simplest CFR estimator. It uses aggreate data of fatalities and cases at a given point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nCFR(outbreak, t):\n",
    "    if t > outbreak.duration:\n",
    "        return np.nan\n",
    "    \n",
    "    return outbreak.cumulative_fatality_curve[t] / outbreak.cumulative_epidemic_curve[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandemic.apply(plot_estimator_curve, nCFR, regions=top10_countries)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_estimator_curve(sars_outbreak, nCFR)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome-controlled Naive CFR\n",
    "\n",
    "As seen on [2005 Guani](http://localhost:8888/notebooks/nbs/literature_review.ipynb#Methods-for-Estimating-the-Case-Fatality-Ratio-for-a-Novel,-Emerging-Infectious-Disease), this estimator will control for the censored data by only considering the resolved cases. \n",
    "\n",
    "A **resolved** case is one that has a known outcome of either *death* or *recovery*, whereas an **active** case is one for which no event has occured as of given time period $t$ (therefore, right-censored).\n",
    "\n",
    "Therefore, the first assumption for this estimator is that the CFR for active cases will be similar in distribution than that of resolved cases.\n",
    "\n",
    "Furthermore, another assumption that needs to be met for this estimator to work well is that \"the hazards of death and recovery at any time $k$ from onset, conditional on an event occuring at time $k$, are proportional\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_controlled_nCFR(outbreak, t):\n",
    "    if t > outbreak.duration:\n",
    "        return np.nan\n",
    "    \n",
    "    return outbreak.cumulative_fatality_curve[t] / (outbreak.cumulative_recovery_curve[t] + outbreak.cumulative_fatality_curve[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandemic.apply(plot_estimator_curve, outcome_controlled_nCFR, regions=top10_countries)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen on [2005 Guani](http://localhost:8888/notebooks/nbs/literature_review.ipynb#Methods-for-Estimating-the-Case-Fatality-Ratio-for-a-Novel,-Emerging-Infectious-Disease). This seems powerful, yet cannot be applied to our case as we are missing patient level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underestimate-Corrected Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "from scipy.special import gamma\n",
    "\n",
    "# this distribution is useful as a baseline for the potential parameters we will encounter\n",
    "# based on some paper\n",
    "def estimate_weibull_parameters(mu=8.9, std=5.4, k_0=1):\n",
    "    def optimization_problem(k):\n",
    "        return (std / mu)**2 - (gamma(1+ 2/k) / gamma(1+ 1/k)) ** 2 + 1;\n",
    "\n",
    "    # solve for k\n",
    "    k = fsolve(optimization_problem, k_0)[0]   \n",
    "    # solve for lambda\n",
    "    l = mu / gamma(1 + 1/k) \n",
    "    \n",
    "    return k, l\n",
    "\n",
    "k, l = estimate_weibull_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known_outcome_adjusted_CFR(outbreak, t):\n",
    "    case_density = outbreak.epidemic_curve[:t]\n",
    "    resolved_case_rate = outbreak.resolved_case_rate[:t]\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    for i in range(t):\n",
    "        for j in range(i + 1):\n",
    "            result += case_density[i - j] * resolved_case_rate[j]\n",
    "            \n",
    "    known_outcome_coefficients = result / case_density.sum()\n",
    "    \n",
    "    return nCFR(outbreak, t) * known_outcome_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandemic.apply(plot_estimator_curve, known_outcome_adjusted_CFR, regions=[\"France\", \"US\", \"Germany\", \"Belgium\"])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epidemic Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def dtw(x, y):\n",
    "    distance, path = fastdtw(x, y, dist=euclidean)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def norm(x, y):\n",
    "    return np.linalg.norm(x - y)\n",
    "\n",
    "def mean_cumulative_absolute_expected_distance(cumulative_realization, simulations):\n",
    "    T = cumulative_realization.shape[0]\n",
    "    cumulative_simulations = simulations.cumsum(axis=1)\n",
    "    \n",
    "    cumulative_absolute_expected_difference = np.abs(cumulative_realization - np.mean(cumulative_simulations, axis=0))\n",
    "    \n",
    "    return np.mean(cumulative_absolute_expected_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import weibull_min\n",
    "\n",
    "def generate_hazard_rate(k, l, ppf_end=0.99):\n",
    "    W = weibull_min(k, scale=l)\n",
    "    K = int(W.ppf(ppf_end))\n",
    "    \n",
    "    x = np.arange(K)\n",
    "    \n",
    "    fatality_rate = W.pdf(x)\n",
    "    cumulative_fatality_rate = W.cdf(x)\n",
    "    \n",
    "    hazard_rate = np.zeros(K)\n",
    "    hazard_rate[0] = fatality_rate[0]\n",
    "    hazard_rate[1:K] = fatality_rate[1:K] / (1 - cumulative_fatality_rate[:K-1])\n",
    "\n",
    "    return hazard_rate\n",
    "\n",
    "def survival_function(t, hazard_rate):\n",
    "    if t > hazard_rate.shape[0]:\n",
    "        raise Exception(\"bad arg\") \n",
    "    \n",
    "    return np.exp(-hazard_rate[:t].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fatality-Only Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fatality_outcomes(n_cases, fatality_probability, fatality_hazard_rate):\n",
    "    K = fatality_hazard_rate.shape[0]\n",
    "    \n",
    "    probability_samples = np.random.rand(n_cases, K + 1)\n",
    "    \n",
    "    fatality_bitmask = probability_samples[:, 0] < fatality_probability\n",
    "    outcome_days = np.argmax(probability_samples[:, 1:] < fatality_hazard_rate, axis=1)\n",
    "    \n",
    "    return outcome_days, fatality_bitmask\n",
    "\n",
    "def simulate_epidemic_fatality_outcomes(cases, fatality_probability, fatality_hazard_rate, n_sims=100):\n",
    "    T = cases.shape[0]\n",
    "    \n",
    "    fatalities = np.zeros((n_sims, T))\n",
    "    \n",
    "    for t, n_cases in enumerate(cases.astype(int)):\n",
    "        outcome_days, fatality_bitmask = simulate_fatality_outcomes(\n",
    "            n_cases * n_sims, \n",
    "            fatality_probability, \n",
    "            fatality_hazard_rate\n",
    "        )\n",
    "        \n",
    "        for sim in range(n_sims):\n",
    "            sim_ptr = n_cases * sim\n",
    "            \n",
    "            sim_outcome_days = outcome_days[sim_ptr: sim_ptr + n_cases]\n",
    "            sim_fatality_bitmask = fatality_bitmask[sim_ptr: sim_ptr + n_cases]\n",
    "            \n",
    "            sim_fatality_days = np.bincount(sim_outcome_days[sim_fatality_bitmask])\n",
    "            censoring = min(sim_fatality_days.shape[0], T - t)\n",
    "            \n",
    "            fatalities[sim, t:(t + censoring)] += sim_fatality_days[:censoring]\n",
    "            \n",
    "    return fatalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dual-Event Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_both_outcomes(n_cases, fatality_probability, fatality_hazard_rate, recovery_hazard_rate):\n",
    "    K = fatality_hazard_rate.shape[0]\n",
    "    \n",
    "    probability_samples = np.random.rand(n_cases, K + 1)\n",
    "    \n",
    "    fatality_bitmask = probability_samples[:, 0] < fatality_probability\n",
    "    \n",
    "    fatality_outcome_days = np.argmax(probability_samples[fatality_probability, 1:] < fatality_hazard_rate, axis=1)\n",
    "    # use the inverse to make sure the two events are exclusive\n",
    "    recorvey_outcome_days = np.argmax(probability_samples[~fatality_probability, 1:] > 1 - recovery_hazard_rate, axis=1)\n",
    "    \n",
    "    # assumption: if fatality/recovery occurs the same day, the fatality takes precendence\n",
    "    # fatality_bitmask = fatality_outcomes <= recovery_outcomes\n",
    "    # outcome_days = np.minimum(fatality_outcomes, recovery_outcomes)\n",
    "    \n",
    "    return fatality_outcome_days, recorvey_outcome_days, fatality_bitmask\n",
    "    \n",
    "def simulate_epidemic_both_outcomes(cases, fatality_probability, fatality_hazard_rate, recovery_hazard_rate, n_sims=100):\n",
    "    T = cases.shape[0]\n",
    "    K = hazard_rate.shape[0]\n",
    "    \n",
    "    fatalities = np.zeros((n_sims, T))\n",
    "    recoveries = np.zeros((n_sims, T)) \n",
    "    \n",
    "    for t, n_cases in enumerate(cases.astype(int)):\n",
    "        fatality_outcome_days, recorvey_outcome_days, fatality_bitmask = simulate_cases(int(n_cases * n_sims), fatality_hazard_rate, recovery_hazard_rate)\n",
    "        n_recoveries = (~fatality_bitmask).sum()\n",
    "        \n",
    "        for sim in range(n_sims):\n",
    "            sim_outcome_days = outcome_days[n_cases * sim: n_cases * (sim + 1)]\n",
    "            sim_fatality_bitmask = fatality_bitmask[n_cases * sim: n_cases * (sim + 1)]\n",
    "            \n",
    "            deaths_per_day = np.bincount(sim_outcome_days[sim_fatality_bitmask])\n",
    "            censoring = min(deaths_per_day.shape[0], T - t)\n",
    "            \n",
    "            fatalities[sim, t:(t + censoring)] += deaths_per_day[:censoring]\n",
    "            \n",
    "    return fatalities, recoveries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import Optimizer, dump, load\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real\n",
    "from joblib import Parallel, delayed\n",
    "from tabulate import tabulate\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutbreakOptimizer:\n",
    "    def __init__(self, outbreak, dimensions, T=None, random_state=1, n_random_starts=20, **kwargs):\n",
    "        self.optimizer = Optimizer(dimensions=dimensions, random_state=random_state, n_random_starts=n_random_starts)\n",
    "        \n",
    "        self.outbreak = outbreak\n",
    "        self.results = {}\n",
    "        \n",
    "        self.T = T\n",
    "        self.evaluator = self.build_evaluator(**kwargs)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_file(region, T=None, resultpath=\"coronavirus\"):\n",
    "        return load(f\"../results/{resultpath}/{region}_optimization_results_{T}.pkl\")\n",
    "        \n",
    "    def censor(self, T, smooth=False, n_sims=1000, distance_metric=mean_cumulative_absolute_expected_distance):\n",
    "        self.T = T\n",
    "        \n",
    "        self.evaluator = self.build_evaluator(**kwargs)\n",
    "        \n",
    "    def build_evaluator(self, smooth=False, n_sims=1000, distance_metric=mean_cumulative_absolute_expected_distance):\n",
    "        if smooth:\n",
    "            cases, real_fatalities = self.outbreak.smooth_epidemic_curve.values, self.outbreak.smooth_fatality_curve.cumsum().values\n",
    "        else:\n",
    "            cases, real_fatalities = self.outbreak.epidemic_curve.values, self.outbreak.fatality_curve.cumsum().values\n",
    "            \n",
    "        # apply censoring\n",
    "        cases, real_fatalities = cases[:self.T], real_fatalities[:self.T]    \n",
    "        \n",
    "        def _evaluator(alpha, k, l):\n",
    "            hazard_rate = generate_hazard_rate(k, l)\n",
    "            fatality_simulations = simulate_epidemic_fatality_outcomes(cases, alpha, hazard_rate, n_sims)\n",
    "        \n",
    "            return distance_metric(real_fatalities, fatality_simulations)\n",
    "        \n",
    "        return _evaluator\n",
    "    \n",
    "    def save(self, resultpath=\"coronavirus\"):\n",
    "        dump(self.optimizer.get_result(), f\"../results/{resultpath}/{self.outbreak.region}_optimization_results_{self.T}.pkl\")\n",
    "    \n",
    "    \n",
    "    def plot_results(self):\n",
    "        ax = plot_objective(self.optimizer.get_result(), minimum='expected_minimum')\n",
    "        ax.title(f\"{self.outbreak.region} - Partial Dependence Plot - T={self.T}\")\n",
    "        plt.show()\n",
    "        \n",
    "    def optimize(self, n_runs, n_jobs=4, verbose=False):\n",
    "        print(f\"Parameter Optimization for {self.outbreak.region} [runs={n_runs}, workers={n_jobs}, T={self.T}]\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for iter_id in range(n_runs):\n",
    "            suggestions = self.optimizer.ask(n_points=n_jobs)\n",
    "            \n",
    "            costs = Parallel(n_jobs=n_jobs)(delayed(self.evaluator)(*x) for x in suggestions)\n",
    "            \n",
    "            self.optimizer.tell(suggestions, costs)\n",
    "            \n",
    "            results += [[iter_id, worker_id, *x, costs[worker_id]] for worker_id, x in enumerate(suggestions)]\n",
    "            \n",
    "            if verbose:\n",
    "                print(tabulate(results, headers=[\"Iteration\", \"Worker\", \"alpha\", \"k\", \"l\", \"cost\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for T in [None, 14, 21, 30, 60]:\n",
    "    sars_opt = OutbreakOptimizer(sars_outbreak, [Real(0.00001, 0.2), Real(1.0, 4.0), Real(1.0, 20.0)], T=T)\n",
    "\n",
    "    sars_opt.optimize(10, n_jobs=4, verbose=True)\n",
    "    \n",
    "    sars_opt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_objective(sars_opt.optimizer.get_result(), minimum='expected_minimum')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sars_outbreak.cumulative_fatality_curve[-1] / sars_outbreak.cumulative_epidemic_curve[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coronavirus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region, outbreak in pandemic.get_outbreaks(top10_countries).items():\n",
    "    for T in [None, 14, 21, 30, 60]:\n",
    "        opt = OutbreakOptimizer(outbreak, [Real(0.000001, 0.25), Real(1.0, 2.2), Real(10.0, 20.0)], T=T)\n",
    "        opt.optimize(10, n_jobs=4)\n",
    "        opt.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(OutbreakOptimizer.from_file(\"Brazil\", T=14))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Brazil\", T=21))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Brazil\", T=30))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Brazil\", T=60))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Brazil\", T=None))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(OutbreakOptimizer.from_file(\"Belgium\", T=14))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Belgium\", T=21))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Belgium\", T=30))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Belgium\", T=60))\n",
    "plot_objective(OutbreakOptimizer.from_file(\"Belgium\", T=None))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_params(region, results):\n",
    "    params = results[region][\"params\"]\n",
    "    \n",
    "    h = generate_hazard_rate(params[\"k\"], params[\"l\"])\n",
    "    fatalities = simulate_epidemic_fatality_outcomes(pandemic.outbreaks[region].smooth_epidemic_curve, params[\"alpha\"], h)\n",
    "    \n",
    "    plt.title(f\"{region} - Simulated deaths\")\n",
    "    plt.plot(pandemic.outbreaks[region].smooth_fatality_curve.values)\n",
    "    plt.plot(fatalities.T, c='grey')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First iteration\n",
    "\n",
    "- no smoothing\n",
    "- static recovery probability\n",
    "- 100 simulations\n",
    "- mean of mean absolute difference between simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_best_parameters = {'k': 1.5680800271994926, 'l': 13.459646608653848, 'r': 0.27727495101632926}\n",
    "france_best_parameters = {'k': 1.5932391692362207, 'l': 13.595494436079711, 'r': 0.2527648604733469}\n",
    "spain_best_parameters = {'k': 1.156937432220853, 'l': 12.429343784489696, 'r': 0.4057687057183451}\n",
    "germany_best_parameters = {'k': 1.3961487078200099, 'l': 17.922213071507144, 'r': 0.7504989403338654}\n",
    "us_best_parameters = {'k': 1.680058976798015, 'l': 15.470903267327513, 'r': 0.458335669097955}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second iteration\n",
    "\n",
    "- no smoothing\n",
    "- 1000 simulations\n",
    "- min of mean absolute difference between simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_space = np.arange(2, 30)\n",
    "p_space = np.linspace(1, 0, 100, endpoint=False)\n",
    "K_space = np.arange(2, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, r_space, p_space, K_space):\n",
    "    r_bar = master_bar(r_space)\n",
    "    p_bar = progress_bar(p_space, parent=r_bar)\n",
    "    \n",
    "    minimum_distance = None\n",
    "    minimum_parameters = None\n",
    "\n",
    "    for r in r_bar:\n",
    "        for p in p_bar:\n",
    "            for K in K_space:\n",
    "                distance = model.evaluate_hazard_rate(r, p, K)\n",
    "\n",
    "                if minimum_distance is None or minimum_distance > distance:\n",
    "                    minimum_distance = distance\n",
    "                    minimum_parameters = (r, p, K)\n",
    "\n",
    "                r_bar.child.comment = f'distance={distance}'\n",
    "\n",
    "        r_bar.main_bar.comment = f\"best_params={minimum_parameters}, minimum_distance={minimum_distance}\"\n",
    "\n",
    "    return minimum_parameters, minimum_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive CFR\n",
    "\n",
    "Firstly, we can analyze the most recent Naive CFR values. We can use these initial values to create a search range for our estimator. We consider both the global distribution as well as the one considering only the top 10 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_recent_cfr = global_coronavirus_outbreak.cfr_curve.iloc[-1]\n",
    "\n",
    "pd.DataFrame({\"World\": most_recent_cfr.describe(), \"Top10\": most_recent_cfr[coronavirus_top10_countries].describe()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, using all countries greatly reduces the mean and quartile values of CFR. This makes sense as a lot of countries are yet to hit their peak of the infection, and so their current CFR is an underestimation of what it will eventually reach (mostly due to the lag between diagnosis and outcome + the pile-on effects of exhausted health service resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-death-adjusted CFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the Naive CFR estimator varies a lot in the early stages and begins to steadily grow as the epidemic develops (towards a flatenning at different levels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch first death time lags from global death df\n",
    "first_death_time_lags = global_death_df.eq(0).sum().apply(lambda x: pd.Timedelta(days=x))\n",
    "\n",
    "ax = plt.gca()\n",
    "k=1\n",
    "\n",
    "for country in coronavirus_top10_countries:\n",
    "    cumulative_country_df = global_coronavirus_outbreak.get_region_df(country).cumsum(axis=0)\n",
    "    \n",
    "    first_death_in_country = first_death_time_lags[country]\n",
    "    corrected_cumulative_country_df = cumulative_country_df.loc[first_death_in_country:, :].reset_index(drop=True)\n",
    "    \n",
    "#     (corrected_country_df.loc[:, \"Dead\"].cumsum() / corrected_country_df.loc[:, \"Infected\"].cumsum()).rolling(k).mean().plot(ax=ax, label=country)\n",
    "\n",
    "    (corrected_cumulative_country_df[\"Dead\"] / corrected_cumulative_country_df[\"Infected\"]).rolling(k).mean().plot(ax=ax, label=country)\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "ax.set_xlabel(\"CFR\")\n",
    "ax.set_xlabel(\"days since first death\")\n",
    "\n",
    "global_coronavirus_outbreak.add_plot_details(ax, f\"Naive CFR - MA={k} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual epidemic study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to convert our indices to time deltas to facilitate manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find first day that confirm cases aren't 0 for every country\n",
    "first_confirmed_case_time_lags = global_confirmed_df.eq(0).sum().apply(lambda x: pd.Timedelta(days=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in global_coronavirus_outbreak.regions:\n",
    "    country_df = global_coronavirus_outbreak.get_region_df(country)\n",
    "    \n",
    "    first_confirmed_case_in_country = first_confirmed_case_time_lags[country]\n",
    "    corrected_country_df = country_df.loc[first_confirmed_case_in_country - pd.Timedelta(days=1):, :]\n",
    "    \n",
    "    corrected_country_df.loc[:, \"CFR\"] = corrected_country_df.loc[:, \"Dead\"] / corrected_country_df.loc[:, \"Infected\"]\n",
    "    \n",
    "    corrected_country_df.to_csv(f\"../data/countries/{country}.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_confirmed_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv') \\\n",
    "                    .drop([\"UID\", \"iso2\", \"iso3\", \"code3\", \"Admin2\", \"Lat\", \"Long_\", \"FIPS\", \"Country_Region\", \"Combined_Key\"], axis=1).groupby('Province_State').sum().transpose()\n",
    "us_death_df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv')\\\n",
    "                .drop([\"UID\", \"iso2\", \"iso3\", \"code3\", \"Admin2\", \"Lat\", \"Long_\", \"FIPS\", \"Country_Region\", \"Combined_Key\", \"Population\"], axis=1).groupby('Province_State').sum().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_coronavirus_outbreak = Outbreak(\"us_coronavirus\")\n",
    "\n",
    "us_coronavirus_outbreak.set_epidemic_curve(us_confirmed_df)\n",
    "us_coronavirus_outbreak.set_fatality_curve(us_death_df)\n",
    "\n",
    "us_coronavirus_outbreak.convert_indices_to_timedelta_since_epidemic_start_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_coronavirus_outbreak.filter_top_regions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_regions_total_number_of_cases = us_coronavirus_outbreak.epidemic_curve.sum(axis=1)[-1]\n",
    "us_total_number_of_cases = us_coronavirus_outbreak.base_epidemic_curve.sum(axis=1)[-1]\n",
    "\n",
    "top_regions_total_number_of_deaths = us_coronavirus_outbreak.fatality_curve.sum(axis=1)[-1]\n",
    "us_total_number_of_deaths = us_coronavirus_outbreak.base_fatality_curve.sum(axis=1)[-1]\n",
    "\n",
    "print(\"Case coverage=%.2f\" % ((top_regions_total_number_of_cases / us_total_number_of_cases) * 100))\n",
    "print(\"Death coverage=%.2f\" % ((top_regions_total_number_of_deaths / us_total_number_of_deaths) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove those regions not yet affected by the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "us_coronavirus_outbreak.cfr_curve.fillna(0).plot(ylim=[-0.01, 0.1], ax=ax)\n",
    "\n",
    "us_coronavirus_outbreak.add_plot_details(ax, \"cfr_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:coronavirus]",
   "language": "python",
   "name": "conda-env-coronavirus-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
